# AI 作业批改 Web 工具 · 开发流程说明（IPO）

## 0. 总览：阶段与目标一览

| 阶段 | 名称        | 目标概述                  |
| -- | --------- | --------------------- |
| P0 | 需求理解      | 把要做什么、做到什么程度说清楚       |
| P1 | 后端初始化     | 起一个能跑的 Python Web 空项目 |
| P2 | 数据库设计     | 定好批次 / 文件 / 评分记录的数据结构 |
| P3 | 公共模块开发    | 日志、配置、通用响应等基础能力       |
| P4 | 核心业务模块开发  | 文件上传、解析、AI 评分、结果导出    |
| P5 | 前端初始化     | 起一个能打开的 Web 页面骨架      |
| P6 | 用户端开发     | 做完教师使用的前端界面与交互        |
| P7 | 管理端开发（轻量） | 可选的批次信息/日志查看入口        |
| P8 | 联调测试      | 前后端串起来，用真实作业测一轮       |
| P9 | 打包部署      | 本地一键启动说明与依赖打包         |

下面按阶段逐个拆成 Input → Process → Output。

---

## P0 需求理解

### Input（输入）

* 项目目标描述：做一个本地可用的 AI 批改工具
* 已整理好的文档：PDR、PRD
* 老师的真实诉求：

  * 批量批改 Word 作业
  * 自动生成分数 + 评语
  * 导出 Excel 给教务系统录分

### Process（过程）

1. 读取 PDR、PRD，梳理关键路径：

   * 上传 docx → 校验解析 → AI 评分 → 导出 Excel & 异常清单
2. 划定边界：

   * 只做本地单机，不搞登录、不搞权限、不搞云部署
   * 只支持 .docx 文件，其他格式视为异常
3. 列出业务核心对象：

   * 批次（Batch）
   * 作业文件（File）
   * 评分记录（GradeRecord）
4. 拆分成可开发的模块：

   * 后端：Web API、文件管理、解析、AI 调用、导出
   * 前端：上传界面、配置输入、结果展示与下载
5. 最终确认需求清单与不做的内容，形成内部任务列表（待办）。

### Output（输出）

* 一份内部任务拆解清单（如：后端模块列表、前端页面元素清单）
* 清晰的范围边界说明（开发期间不再随便加功能）
* 对“成功”的定义：能批量处理一批 docx 并导出正确的成绩表和异常清单

---

## P1 后端初始化

### Input（输入）

* 阶段 P0 输出的需求与模块列表
* 约束条件：Python + Web，本地运行，轻量

### Process（过程）

1. 创建项目目录结构示意：

   * app/

     * api/（各接口模块）
     * service/（业务逻辑）
     * model/（数据模型）
     * util/（工具类）
   * config/（配置文件）
   * data/（后续存批次文件等）
   * main.py（启动入口）
2. 选择 Web 框架（如 Flask 或 FastAPI），初始化最小可运行服务：

   * 创建一个健康检查接口，如 GET /ping 返回简单 JSON
3. 建立虚拟环境并整理依赖：

   * Web 框架
   * python-docx
   * openpyxl/xlsxwriter
   * requests/httpx
4. 启动本地服务，确认浏览器能访问到一个简单返回。

### Output（输出）

* 一个可启动的 Python Web 项目骨架
* 一个能在浏览器里访问的基础接口（证明后端环境正常）
* 初版依赖清单（requirements.txt）

---

## P2 数据库设计

### Input（输入）

* 核心业务对象：批次、文件、评分记录
* 访问模式：本地单用户，读写压力小
* 不需要复杂数据库集群的前提

### Process（过程）

1. 选型 SQLite 作为本地存储（可选，用于记录历史批次）
2. 设计表结构：

   * grading_batch：批次表
   * grading_file：文件表
   * grading_record：评分记录表
3. 编写建表语句或 ORM 模型：

   * 为每个实体定义字段类型与约束
4. 编写数据库初始化脚本：

   * 若数据库文件不存在，自动创建并建表
5. 在后端项目中增加数据库连接管理（简单连接池或按请求打开）。

### Output（输出）

* 数据库结构设计说明（字段含义清晰）
* 初始化好的 SQLite 数据库文件
* 在代码中可调用的数据库访问层（读写批次、文件、评分记录）

> 注：如果你嫌麻烦，也可以暂时不用数据库，所有信息只存在 Excel 和日志文件中，但设计文档里数据库是有的，很容易以后补上。

---

## P3 公共模块开发

### Input（输入）

* 已初始化的后端项目与数据库设计
* 后续核心业务模块对「日志、配置、通用响应」的共同需求

### Process（过程）

1. 配置管理模块：

   * 从本地配置文件（如 config.yaml）读取：

     * 数据存储目录根路径
     * 日志路径
   * 提供统一配置访问方法
2. 日志模块：

   * 使用 Python logging 封装：

     * 记录 INFO / ERROR 级别日志
     * 按日期滚动或单文件输出
3. 通用响应封装：

   * 定义标准 API 返回结构：

     * code（0 成功，非 0 失败）
     * message（说明）
     * data（数据）
   * 后端接口统一使用此结构返回
4. 错误码与异常处理：

   * 约定常见错误码：

     * 1001：文件解析错误
     * 1002：模型调用错误
     * 1003：参数错误
   * 设置全局异常捕获，中断时返回明确错误信息给前端。

### Output（输出）

* 可复用的配置类 / 方法
* 项目级日志能力（运行与报错可查）
* 统一的 HTTP 返回格式与错误处理方式

---

## P4 核心业务模块开发

这里是整个系统的心脏，替代「用户/商品/购物车/订单/评价」那套，换成你真正要干的东西。

### P4.1 文件上传与批次管理模块

#### Input（输入）

* 前端上传的多份 docx 文件
* 当前的模型配置（与批次信息一起记录）

#### Process（过程）

1. 接收 multipart/form-data 上传请求
2. 为本次操作生成唯一 batchId
3. 在本地创建对应目录：

   * data/uploads/{batchId}/
4. 将每个文件保存到该目录
5. 解析文件名，尝试提取学号、姓名
6. 写入数据库或内存结构：

   * grading_batch：批次记录
   * grading_file：每个文件一条记录，状态为“未处理”

#### Output（输出）

* 新批次的 batchId
* 每个文件的存储路径与基础元信息
* 前端收到批次 ID，可用于后续启动评分

---

### P4.2 文件解析与校验模块

#### Input（输入）

* 指定 batchId 对应的文件列表（grading_file 或文件清单）

#### Process（过程）

1. 遍历批次内所有文件
2. 对每个文件执行：

   * 检查扩展名是否为 .docx
   * 使用 python-docx 尝试打开
   * 若无法打开或抛异常：

     * 标记 parse_status = FORMAT_ERROR / PARSE_ERROR
     * 记录错误信息
   * 若成功打开：

     * 遍历段落拼接正文文本
     * 计算正文字数，若过短则视为 EMPTY_CONTENT
3. 将“可用正文”的文件加入可处理列表，其他加入异常列表
4. 更新 grading_file 的解析状态与错误信息。

#### Output（输出）

* 可解析文件列表（含正文文本）
* 异常文件列表（含错误原因）
* 更新后的文件状态（用于后续导出异常清单）

---

### P4.3 AI 内容评分模块

#### Input（输入）

* 可解析文件列表（文件元信息 + 正文文本）
* 模型配置（API URL、API Key、模型名称、评分模版类型）

#### Process（过程）

1. 根据评分模版构造 Prompt：

   * 作业类型说明
   * 评分规则（总分 + 维度）
   * 输出必须为 JSON 的要求
   * 拼接学生正文
2. 使用 HTTP 客户端请求大模型 API：

   * 设置超时
   * 失败时根据策略进行有限次数重试
3. 收到模型返回内容后：

   * 截取 JSON 片段（如果前后多了一堆废话）
   * 尝试解析为预期结构：score、comment、dimensions
   * 解析失败则标记为 MODEL_ERROR
4. 将成功解析的结果封装为 GradeRecord：

   * 绑定文件 ID、batchId
   * 总分、各维度分、评语、正文字数、状态
5. 将所有 GradeRecord 写入数据库或临时结构。

#### Output（输出）

* 每个可处理文件对应的一条评分记录
* 评分成功和评分失败（模型问题）的清单
* 可供后续导出模块直接使用的 GradeRecord 列表

---

### P4.4 结果汇总与导出模块

#### Input（输入）

* BatchId 对应的：

  * grading_file（包含异常信息）
  * grading_record（包含评分信息）

#### Process（过程）

1. 生成成绩表 Excel：

   * 一行一份作业记录
   * 列包括：

     * 学号、姓名、文件名
     * 总分、各维度分
     * 正文字数
     * 状态、错误信息
2. 生成异常清单 Excel / CSV：

   * 列包括：文件名、错误类型、错误描述
3. 将文件保存到本地指定目录：

   * data/output/{batchId}/grade_result.xlsx
   * data/output/{batchId}/error_files.xlsx
4. 将下载地址通过 API 返回给前端。

#### Output（输出）

* 当前批次的成绩表下载链接
* 当前批次的异常清单下载链接
* 统计信息（总数、成功数、异常数、平均分）供前端展示

---

## P5 前端初始化

### Input（输入）

* 已经跑起来的后端基础 API（健康检查）
* 交互流程：上传 → 配置 → 启动 → 等待 → 下载

### Process（过程）

1. 创建前端目录结构：

   * static/ 或 frontend/
   * index.html（单页即可）
   * simple.css（样式）
   * main.js（交互逻辑）
2. 在 index.html 中预留区域：

   * 文件上传区
   * 模型配置区
   * 执行按钮区
   * 状态/结果展示区
3. 用原生 JS 写一个简单的 Ajax 调用后端 /ping，确认前后端联通。

### Output（输出）

* 能在浏览器打开的空壳页面
* 前端能调用后端基础接口，验证通讯正常

---

## P6 用户端开发（教师使用界面）

### Input（输入）

* 前端骨架页面
* 后端已实现的上传、启动批改、结果查询/下载 API

### Process（过程）

1. 完成文件上传区域：

   * 使用 input[type="file"] 多选
   * 显示选中文件列表（名称 + 大小）
   * 点击“上传并创建批次”，调用 /api/upload-files，获取 batchId
2. 完成模型配置区域：

   * 输入框：API URL、API Key、模型名称
   * 下拉框：评分模版类型
   * 提交前进行前端校验（不能为空）
3. 完成“开始批改”按钮逻辑：

   * 调用 /api/start-grading，携带 batchId 与模型配置
   * 请求过程中禁用按钮、防止重复点击
4. 完成状态展示：

   * 显示“处理中，请稍候”
   * 接收后端返回的统计信息：

     * 总文件数、成功数、异常数、平均分
5. 完成结果下载区域：

   * 显示“下载成绩表”“下载异常清单”两个按钮
   * 点击后直接访问对应下载接口
6. 配置记忆：

   * 使用 localStorage 记住 API URL / 模型名称 / 上次评分模版
   * 页面加载时自动回填这些值

### Output（输出）

* 一个教师可以完整操作的 Web 界面：

  * 能上传文件
  * 能配置模型
  * 能启动批改
  * 能查看批量结果
  * 能下载 Excel

---

## P7 管理端开发（轻量，可选）

本系统不做复杂“后台管理”，但保留一个简易的“批次列表查看”能力，方便你自己排查问题。

### Input（输入）

* 数据库中的批次、文件、评分记录数据
* 简单的内部查看需求（给你自己或老师展示用）

### Process（过程）

1. 在后端增加只读 API：

   * GET /api/batches：返回最近若干批次列表
   * GET /api/batch/{batchId}/summary：返回某批次的统计数据
2. 在前端增加一个简单“历史记录”页面或区域：

   * 列表展示：批次 ID、日期、文件数、平均分、异常数
3. 不提供修改、删除操作，只做查看。

### Output（输出）

* 一个简易的“历史批次查看”能力（管理端极简版本）
* 便于查日志、查某次批次出了什么问题

---

## P8 联调测试

### Input（输入）

* 后端所有核心模块代码
* 前端完整页面
* 准备好的测试样例：

  * 正常 docx 作业若干
  * 损坏 docx / 伪装文件
  * 正文极短的文档

### Process（过程）

1. 单接口测试：

   * 用 Postman / curl 调用后端各 API，确认返回结构正确
2. 端到端流程测试：

   * 前端上传测试作业 → 启动批改 → 下载结果
3. 异常场景测试：

   * 模型 API Key 故意填错，确认错误提示
   * 上传包含异常文件的批次，确认异常清单内容
4. 性能小测：

   * 用 30～50 份作业跑一轮，观察耗时与稳定性
5. 记录发现的问题，逐项修复，回归测试。

### Output（输出）

* 一套通过核心场景的系统：

  * 主流程无错误
  * 常见异常能被识别和提示
* 问题清单与修复状态记录

---

## P9 打包部署

### Input（输入）

* 已经通过联调测试的代码
* 本地运行环境（Python 版本约定）

### Process（过程）

1. 整理依赖：

   * 更新 requirements.txt，确保包含所有库
2. 编写运行说明文档（给老师/自己看的）：

   * 如何创建虚拟环境
   * 如何安装依赖
   * 如何启动服务（示例命令）
   * 如何在浏览器访问（例如 [http://127.0.0.1:9527）](http://127.0.0.1:9527）)
3. 可选：写一个简单启动脚本：

   * 双击脚本启动后端服务
4. 检查目录：

   * data/ 目录存在并有读写权限
   * 日志目录可用

### Output（输出）

* 一份包含以下内容的“可交付包”：

  * 源码目录
  * requirements.txt
  * 启动脚本（如有）
  * 使用说明文档（README）
* 任何人在装好 Python 后，按说明即可在本地启动并使用该系统