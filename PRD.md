# 一、项目名称

* 中文名称：AI 作业批改 Web 工具

---

# 二、系统定位

本系统是一个基于 Python + Web 的本地轻量级工具，为教师或助教提供「批量导入 Word 作业 → 自动调用大模型评分 → 生成成绩表与异常清单 → 下载导出」的一站式流程。

特点：

* 部署形态：本机运行的 Python Web 服务 + 浏览器前端页面
* 使用方式：教师打开浏览器访问本机地址即可使用
* 设计原则：

  * 能跑、能用、逻辑清晰
  * 不做上线部署、云端平台级的复杂设计

---

# 三、用户角色

1. 授课教师

   * 使用目的：快速完成班级作业的批量评分与评语生成，导出 Excel 做成绩录入或存档。
   * 特点：技术水平一般，操作路径必须简单直接。

2. 助教 / 学习委员

   * 使用目的：替老师执行批改工作，整理结果并交给老师。
   * 特点：可以接受稍微多一点操作步骤，但依然需要简单清晰。

不设计其他角色（无管理员、无超级用户等概念）。

---

# 四、功能场景

## 场景 1：批量作业评分与导出成绩表（主路径）

1. 用户打开浏览器访问工具首页。
2. 用户在页面中选择一批 `.docx` 作业文件（多选上传）。
3. 用户填写或确认大模型 API 地址、API Key、评分类型（如“职业规划作业”）。
4. 用户点击「开始批改」。
5. 系统执行：

   * 校验 & 解析所有上传的 docx 文件
   * 对可解析的正文逐份调用大模型进行评分
   * 汇总生成成绩表（Excel）和异常文件列表
6. 批改结束后，页面显示：

   * 总文件数、成功评分数、异常文件数、平均分
   * 提供「下载成绩表」和「下载异常清单」按钮
7. 用户下载 Excel 文件，进行后续成绩录入或存档。

## 场景 2：查看异常文件并通知学生重交

1. 用户在批改完成后，下载异常清单文件。
2. 清单中显示每个异常文件的：文件名、错误原因（如格式损坏、无法解析、正文为空等）。
3. 用户根据文件名确认学生身份，并通知相关学生重新提交作业。

## 场景 3：重复使用工具处理新的作业批次

1. 用户无需重新安装系统，只需要再次打开网页。
2. 默认已记住上次填写的 API 地址、模型名称等（前端 localStorage）。
3. 用户直接选择新一批作业文件，重复执行「开始批改 → 下载结果」流程。

---

# 五、功能点清单（含优先级）

优先级定义：

* P0：当前版本必做，工具不可或缺
* P1：当前版本建议做，提升易用性
* P2：可以后续迭代再做

## 5.1 Web 界面与基础交互

1）上传作业文件（P0）

* 多文件选择上传（仅限 .docx）
* 显示已选文件列表（文件名、大小）
* 对非 .docx 文件给出提示并拒绝上传

2）模型配置输入（P0）

* 输入项：API URL、API Key、模型名称（可选）、评分类型（下拉）
* 提示说明：在输入框附近给简短说明文字
* 前端对必填项进行校验

3）批改控制按钮（P0）

* 「开始批改」按钮
* 禁止在执行中重复点击（自动禁用按钮）

4）进度与状态展示（P1）

* 显示当前批次总文件数
* 显示已处理文件数
* 显示当前状态文案（解析中 / 调用模型中 / 生成成绩表中）

5）结果信息展示（P0）

* 显示当前批次的统计数据：总文件数、成功评分数、异常文件数、平均分
* 提供两个下载按钮：成绩表、异常清单

6）配置记忆（P1）

* 将用户输入的 API URL、模型名称等存储于浏览器 localStorage
* 下次打开页面自动带出

## 5.2 文件处理与校验

1）批次 ID 创建（P0）

* 为每次操作生成唯一批次 ID，用于后端目录和日志标识

2）多文件上传与保存（P0）

* 接收前端上传的文件，保存到本地临时目录（按批次划分）

3）docx 格式校验（P0）

* 检查文件扩展名
* 使用 Python 库尝试打开文档，失败则标记为异常

4）正文文本抽取（P0）

* 从 docx 中提取段落文本
* 合并为单一字符串用于评分

5）基本内容校验与过滤（P1）

* 判断文本长度，过短则标记为内容异常
* 可选：过滤明显无意义内容（空行、多余符号）

## 5.3 AI 内容评分

1）评分 Prompt 构造（P0）

* 按统一模板构造评分请求内容：

  * 作业类型说明
  * 评分维度说明
  * 要求输出为 JSON
  * 拼接学生正文

2）大模型 API 调用（P0）

* 通过 HTTP POST 调用外部模型服务
* 请求体包含：prompt、模型名称、API Key 等必要参数
* 支持超时控制与基础重试机制

3）评分结果解析（P0）

* 期望模型返回标准 JSON：score、comment、dimensions
* 若返回内容有多余包裹文本，尝试截取 JSON 段并解析
* 解析失败则记录为模型错误

4）评分记录生成（P0）

* 为每份作业生成 GradeRecord：

  * 文件名、学号、姓名（从文件名解析，如有）
  * 总分
  * 维度分（结构/内容/表达）
  * 正文字数
  * 评语
  * 状态 & 错误信息

## 5.4 结果导出与下载

1）成绩表生成（P0）

* 使用 Excel 文件输出一条记录对应一行
* 必含字段：学号、姓名、文件名、总分、各维度分、正文字数、状态、错误信息

2）异常清单生成（P0）

* 输出异常文件列表的 Excel 或 CSV
* 字段包含：文件名、错误类型、错误描述

3）下载接口（P0）

* 后端为当前批次生成固定文件名
* 前端提供下载按钮，点击后通过 HTTP 下载

## 5.5 日志与统计

1）批次日志记录（P1）

* 记录每个批次的核心信息到文本日志或数据库

2）错误日志记录（P1）

* 对文件解析错误、模型调用错误等写入后台日志

---

# 六、项目边界

为防止“越写越成平台”，明确本项目不做的内容：

1. 不做用户体系

   * 无登录、注册、权限控制
   * 无多角色管理后台

2. 不做线上部署/云平台

   * 不写云服务器、负载均衡、集群等设计
   * 仅考虑在单机（个人电脑）上运行 Python Web 服务

3. 不做复杂业务分析与报表

   * 不提供图形化报表、图表大屏
   * 不做知识点统计、学习画像分析等高阶功能

4. 不做多格式、多模态输入

   * 当前版本只支持 .docx
   * 不处理 PDF、图片扫描件、音频、视频等

5. 不做高并发、多用户同时批改

   * 默认一台机器一次一个批次
   * 不考虑多用户同时操作冲突问题

6. 不做第三方系统对接

   * 不直接对接教务系统、成绩管理系统
   * 不提供对外回调或 Webhook

---

# 七、验收标准（用于答辩 / 展示）

从「能不能用」这个角度验收，而不是「工程多豪华」。

## 7.1 功能性验收

1）批量评分流程完整通过

* 给定一批包含 10–30 份 docx 作业：

  * 能成功上传
  * 能启动批改
  * 能最终生成成绩表与异常清单

2）异常文件识别

* 至少准备以下 3 类异常样例：

  * 损坏/伪装的 docx 文件
  * 内容极短的文件
  * 模型调用故意失败的文件（错误 API Key）
* 系统需将这些文件标记为异常，并在异常清单中给出合理错误说明。

3）成绩表内容正确

* 成绩表中每一行记录包含：

  * 对应的文件名
  * 正确映射的学号/姓名（若文件名中包含）
  * 合理的分数与文字评语
  * 状态（成功/失败）与错误说明（如有）

## 7.2 易用性验收

1）操作流程清晰

* 非技术背景的教师在简单说明下，能够：

  * 独立完成上传、配置、启动批改和下载结果
  * 不需要记复杂命令

2）界面提示明确

* 对常见问题如：未填写 API、未上传文件、模型报错、解析失败等，均有清晰的提示文案。

## 7.3 稳定性验收

1）单次批改不少于 50 份作业

* 在 50 份作业规模下，系统运行不中断，不崩溃。

2）错误隔离

* 即使某个文件解析出错或模型返回异常，也不会影响其他文件的评分流程。

## 7.4 技术合理性验收

1）结构清晰

* 代码层面按模块划分：文件处理、模型调用、结果导出等有清晰界限。

2）符合“本地可运行”约束

* 所有依赖通过 pip 安装即可
* 启动方式简单（如：python main.py，然后浏览器访问指定地址）
